{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQOrgRUEfBFr0Zbja+/wwN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masaers/workshop-numpy/blob/main/pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gN1WufysUpv"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import geopy.distance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch data\n",
        "We will fetch data about all weather stations in Sweden that have ever reported air temperature, and put them in a Pandas data frame for future reference. By separating the raw data (`fetched_df`) from the data we are trying to process (`df`), we can go back to the raw data without having to refetch it."
      ],
      "metadata": {
        "id": "R4hWj04oSAvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://opendata-download-metobs.smhi.se\"\n",
        "parameter = 1 # Air temperature\n",
        "url = f\"{base_url}/api/version/1.0/parameter/{parameter}/station.json\"\n",
        "fetched_df = pd.json_normalize(requests.get(url).json()[\"station\"])"
      ],
      "metadata": {
        "id": "3IW90Ry1sYy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What do we have?\n",
        "\n",
        "We make a copy of the raw data and take a look at it. Both the values and their data types (`dtype`) are interesting."
      ],
      "metadata": {
        "id": "PF6AYiuqSdA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = fetched_df.copy()\n",
        "display(df)\n",
        "display(df.dtypes)"
      ],
      "metadata": {
        "id": "b-RC0NuX3OJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Redundant data\n",
        "\n",
        "1. It looks like `key` and `id` are the same value, but in string (`dtype=object`) and `int64` form, so we should remove `key`.\n",
        "\n",
        "2. It also looks like `summary` is some kind of text representation of the record, that we could re-generate from the other data, so we should remove it.\n",
        "\n",
        "3. The `link` field does contain unique data, but not the kind we can process as \"data\" (and it can be generated by knowing the API), so we will remove that as well.\n",
        "\n",
        "4. Finally, the `title` field is a combination of the `name` and the parameter that we requested via the API, so it does not add any meaningful information and should be removed."
      ],
      "metadata": {
        "id": "y56ygEiBStxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[[c for c in df.columns if c not in [\"key\", \"summary\", \"link\", \"title\"]]]\n",
        "display(df)\n",
        "display(df.dtypes)"
      ],
      "metadata": {
        "id": "2TsAZxMftpMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index\n",
        "\n",
        "The `id`/`key` column was clearly intended as a unique identifier. Let's use it as index!"
      ],
      "metadata": {
        "id": "IjOCmRwGHQn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.set_index(\"id\", drop=True).sort_index()\n",
        "display(df)\n",
        "display(df.dtypes)"
      ],
      "metadata": {
        "id": "jBr4sOCKHR1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversions\n",
        "\n",
        "The conversion from JSON to Python did a good job of converting the types as we got them, but the `updated`, `from` and `to` fields are actually timestamps, so let's use Pandas to interpret the numerical representation correctly.\n",
        "\n",
        "Notice that the type changes to `datetime64[ns]`, a type that is native to Numpy :-)"
      ],
      "metadata": {
        "id": "S_WsCHzGUhhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for c in [\"updated\", \"from\", \"to\"]:\n",
        "    df[c] = pd.to_datetime(df[c], unit=\"ms\")\n",
        "display(df)\n",
        "display(df.dtypes)"
      ],
      "metadata": {
        "id": "-KnC8S-A0hUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categories\n",
        "\n",
        "It also looks like there is some categorical data (`owner`, `ownerCategory` and `measuringStations`). Let's go ahead and convert it to categories!"
      ],
      "metadata": {
        "id": "xmzrNsbAVaL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for c in [\"owner\", \"ownerCategory\", \"measuringStations\"]:\n",
        "    df[c] = df[c].astype(\"category\")\n",
        "display(df)\n",
        "display(df.dtypes)"
      ],
      "metadata": {
        "id": "j5kocNAQ3zPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic statistics\n",
        "\n",
        "Very easy to obtain empirical distributions!\n",
        "\n",
        "Why do we not get any standard deviation for timestamps?"
      ],
      "metadata": {
        "id": "snxL1OAWVw6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "srmQ0K4N1NAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"updated\"].std()"
      ],
      "metadata": {
        "id": "TteAoqTi1zwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking at the data\n",
        "\n",
        "Easy to plot!\n",
        "\n",
        "Let's start by looking at the active time span of stations."
      ],
      "metadata": {
        "id": "0M3bEV3CVHOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot.scatter(\"to\", \"from\")"
      ],
      "metadata": {
        "id": "k915B0YvYr4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hard to tell active from recently decommissioned, let's color the dots according to the `active` column value."
      ],
      "metadata": {
        "id": "GteyGbEkZANg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "color = df[\"active\"].apply(lambda active: \"green\" if active else \"red\")\n",
        "df.plot.scatter(\"to\", \"from\", c=color)"
      ],
      "metadata": {
        "id": "PHGeqIUZVOb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df[\"from\"] == df[\"from\"].min()]"
      ],
      "metadata": {
        "id": "XRrrvfQH8rmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df[\"to\"] == df[\"to\"].min()]"
      ],
      "metadata": {
        "id": "fFr8P4Hr9Bpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do histograms easily as well. Let's plot histograms over positions!"
      ],
      "metadata": {
        "id": "FA1uhIVQZnK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"height\"].plot.hist()\n",
        "plt.show()\n",
        "df[\"latitude\"].plot.hist(bins=100, color=\"green\")\n",
        "plt.show()\n",
        "df[\"longitude\"].plot.hist(bins=100, color=\"red\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DBHtlZU9sFJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we plot a \"map\"? Kinda!"
      ],
      "metadata": {
        "id": "8DVb5TGRaNki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot.scatter(\"longitude\", \"latitude\").set_aspect(1)"
      ],
      "metadata": {
        "id": "fHMggJW75yFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Break it down!\n",
        "\n",
        "We can group by column values and aggregate associated values.\n",
        "\n",
        "The `observed=True` (or `False`) is only needed to avoid a warning."
      ],
      "metadata": {
        "id": "UB8Twenba47L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"owner\", \"height\", \"latitude\", \"longitude\"]].groupby(\"owner\", observed=True).agg([\"min\", \"mean\", \"max\"])"
      ],
      "metadata": {
        "id": "JFOE0Q122NTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like some of the owners have exactly one station... let's verify that by adding a count. This requires different aggregations, so we can take this opportunity to systematize a bit."
      ],
      "metadata": {
        "id": "1YINDL0JlFqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group = [\"owner\"]\n",
        "agg = { c: [\"min\", \"mean\", \"max\"] for c in [\"height\", \"latitude\", \"longitude\"] }\n",
        "agg[\"owner\"] = \"count\"\n",
        "display(agg)\n",
        "columns = list(set(group + list(agg.keys())))\n",
        "df[columns].groupby(group, observed=True).agg(agg)"
      ],
      "metadata": {
        "id": "pgKjrLt_drCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can \"fix\" this by clumping low frequency owners togeather as a new `OTHER` category."
      ],
      "metadata": {
        "id": "g-2LxhOTljpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df[\"owner\"].cat.categories)\n",
        "df[\"owner\"] = df[\"owner\"].cat.add_categories(\"OTHER\")\n",
        "display(df[\"owner\"].cat.categories)"
      ],
      "metadata": {
        "id": "WOzzsfP4khOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I got this recipe off of StackOverflow, so I want to run each step of it to see what it does, so that I understand why it works."
      ],
      "metadata": {
        "id": "d57q4GBhAToV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df[\"owner\"].value_counts())\n",
        "display(df[\"owner\"].value_counts()[df[\"owner\"]])\n",
        "display(df[\"owner\"].value_counts()[df[\"owner\"]].values)\n",
        "display(df.loc[df[\"owner\"].value_counts()[df[\"owner\"]].values < 5, \"owner\"])"
      ],
      "metadata": {
        "id": "-V-2MDsX_APf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From StackOverflow https://stackoverflow.com/a/22208838/1017986\n",
        "df.loc[df[\"owner\"].value_counts()[df[\"owner\"]].values < 5, \"owner\"] = \"OTHER\"\n",
        "display(df[df[\"owner\"] == \"OTHER\"])"
      ],
      "metadata": {
        "id": "OKHf4lMbKogI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group = [\"owner\"]\n",
        "agg = { c: [\"min\", \"mean\", \"max\"] for c in [\"height\", \"latitude\", \"longitude\"] }\n",
        "agg[\"owner\"] = \"count\"\n",
        "columns = list(set(group + list(agg.keys())))\n",
        "df[columns].groupby(group, observed=True).agg(agg)"
      ],
      "metadata": {
        "id": "t3XxQbsReZCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df[\"height\"] < 0]"
      ],
      "metadata": {
        "id": "5HykglGvBQvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group = [\"owner\", \"active\", \"ownerCategory\"]\n",
        "agg = { c: [\"min\", \"mean\", \"max\"] for c in [\"height\", \"latitude\", \"longitude\"] }\n",
        "agg[\"owner\"] = \"count\"\n",
        "columns = list(set(group + list(agg.keys())))\n",
        "df[columns].groupby(group, observed=True).agg(agg)"
      ],
      "metadata": {
        "id": "P8Ix6rug9bPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sorting\n",
        "\n",
        "How far away are these stations from us?"
      ],
      "metadata": {
        "id": "wEN9LOh1oB9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HIQ_STO_LATLON = (59.319268, 18.075664)\n",
        "df[\"from_here\"] = df.apply(lambda row: geopy.distance.distance(HIQ_STO_LATLON, (row[\"latitude\"], row[\"longitude\"])).km, axis=1)\n",
        "display(df[df[\"active\"]].sort_values(by=\"from_here\"))"
      ],
      "metadata": {
        "id": "RHA_NzN897jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My favorite algorithm!\n",
        "\n",
        "My favorite algorithm is [*k*-means](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.vq.kmeans.html) clustering. Let's reimagine Sweden's 21 \"regioner\" based on how weather stations are distributed!\n",
        "\n",
        "Time to break out SciPy!"
      ],
      "metadata": {
        "id": "TpBijn-GodF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = df[[\"longitude\", \"latitude\"]]\n",
        "print(f\"{x.shape=}\")\n",
        "centroids, distortion = scipy.cluster.vq.kmeans(x, 21)\n",
        "print(f\"{centroids.shape=} {distortion=}\")"
      ],
      "metadata": {
        "id": "FXqbH_wR7aMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Show on \"map\"\n",
        "\n",
        "SciPy also let's us construct and graph Voroni tesselations!"
      ],
      "metadata": {
        "id": "6co6YYfNpNGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot.scatter(\"longitude\", \"latitude\", color=\"pink\").set_aspect(1)\n",
        "voroni = scipy.spatial.Voronoi(centroids)\n",
        "scipy.spatial.voronoi_plot_2d(voroni, plt.gca(), show_vertices=False, line_colors=\"red\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t-6knwLG8HyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seeding centroids?\n",
        "\n",
        "Can we do better by seeding *k*-means with current regions?\n",
        "\n",
        "I found a CSV with the location of Swedish cities with Municipality (kommun?) and County (region?) as well as longitude/latitude coordinates on GitHub. Turns out you can just read a URL!\n",
        "\n",
        "No population data, but we can still use the median city in each region as initial centroid."
      ],
      "metadata": {
        "id": "mZODS2HMFd5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cities_df = pd.read_csv(\"https://raw.githubusercontent.com/sphrak/svenska-stader/refs/heads/master/src/svenska-stader.csv\")\n",
        "display(cities_df)\n",
        "display(cities_df[[\"County\", \"Longitude\", \"Latitude\"]].groupby(\"County\").median())\n",
        "display(cities_df[[\"County\", \"Longitude\", \"Latitude\"]].groupby(\"County\").median().values)"
      ],
      "metadata": {
        "id": "Fuw26k73q2Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df[[\"longitude\", \"latitude\"]]\n",
        "guess = cities_df[[\"County\", \"Longitude\", \"Latitude\"]].groupby(\"County\").median().values\n",
        "centroids, distortion = scipy.cluster.vq.kmeans(x, guess)"
      ],
      "metadata": {
        "id": "SD1aIyryEjXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot.scatter(\"longitude\", \"latitude\", color=\"pink\").set_aspect(1)\n",
        "voroni = scipy.spatial.Voronoi(centroids)\n",
        "scipy.spatial.voronoi_plot_2d(voroni, plt.gca(), show_vertices=False, line_colors=\"red\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4x8u9oxMFOL0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}